{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdec1fcf-add6-40ab-9b88-422a25704235",
   "metadata": {},
   "source": [
    "Spacy is a python library that parses \"understand\" lange volume of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be90b980-0408-43e5-8e64-b897d2b42671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy and load the language library\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f02a0f-04a2-4327-a581-5e782202cb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN\n",
      "is AUX\n",
      "lookin NOUN\n",
      "at ADP\n",
      "buying VERB\n",
      "U.S. PROPN\n",
      "startup NOUN\n",
      "for ADP\n",
      "$ SYM\n",
      "6 NUM\n",
      "million NUM\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "#Create a dot object\n",
    "doc=nlp(u'Tesla is lookin at buying U.S. startup for $6 million')\n",
    "\n",
    "#print eack token seperately\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d44ddca-5fcb-453a-b608-6a5fbc9afdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX ROOT\n",
      "lookin NOUN attr\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.S. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "6 NUM compound\n",
      "million NUM pobj\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "#Create a dot object\n",
    "doc=nlp(u'Tesla is lookin at buying U.S. startup for $6 million')\n",
    "\n",
    "#print eack token seperately\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_,token.dep_)\n",
    "    #predict syntactic dependencies\n",
    "    #predicting syntactic dependencies involves identifying the grammatical structure of a sentence \n",
    "    #by determining how different words relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38145f94-98c3-44eb-ad69-0983e3ad93c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1e2d9ad2bd0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1e2d9ad12b0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1e2d8942a40>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1e2d9ad7150>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1e2d9ad4050>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1e2d89429d0>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae6c83c-4af8-48e7-a1fb-7eda62298127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eac073e-e3cf-4eff-ace0-f3807b55aa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "      SPACE dep\n",
      "looking VERB ROOT\n",
      "into ADP prep\n",
      "startups NOUN pobj\n",
      "anymore ADV advmod\n"
     ]
    }
   ],
   "source": [
    "doc2=nlp(u\"Tesla isn't      looking into startups anymore\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text,token.pos_,token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70054953-e10e-4aa1-bfb8-70e4a11baf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tesla isn't      looking into startups anymore"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d862944-4822-456d-9555-fc24fe7819af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tesla"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e13c509d-6199-403e-9024-d6302c8ca07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos of individual token\n",
    "doc2[0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e22f92e9-da12-4cd8-8949-1e80698a667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spacy.io/usage/linguistic-features/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36d6009b-7ce3-4ce8-a838-62285f25e27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nsubj'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dep of individual token\n",
    "doc2[0].dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "681d0aec-8e4d-49f2-88da-6ad1adcc97a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#28/08/24\n",
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c0879b5-3716-434e-8839-f78074bcdcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0709496-05e3-40c5-9d84-813cb883cd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking\n",
      "look\n"
     ]
    }
   ],
   "source": [
    "#Lemmas (the base form of the word):\n",
    "print(doc2[4].text)\n",
    "print(doc2[4].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d37b2c-19dc-4032-956c-7560eb8efc98",
   "metadata": {},
   "source": [
    "\n",
    "Span :\n",
    "\n",
    "Large Doc objects can be hard to work with at times.A span is a slice of Doc object in the form Doc(start.stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "608fce7f-fbf6-46f8-8b34-a4dd172214b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3=nlp(u'Although commonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n",
    "         the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n",
    "         cartonist Allen Saunders and published in Reader\\'s Diggest 1957, When Lennon was 17.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0b855c2a-39eb-4c4a-92e4-05138e43bc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Life is what happens to us while we are making other plans\"\n"
     ]
    }
   ],
   "source": [
    "life_quote=doc3[17:31]\n",
    "print(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce2006e6-b295-4e85-9030-2e4e51f42236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e1fe366-34ea-4f97-a823-12de6069a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4=nlp(u'This is the first sentence.This is another sentence.This is the last sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c8ab9-7277-4959-9ad6-0abb6482b6f9",
   "metadata": {},
   "source": [
    "#SENTENCE\n",
    "Certain tokens inside Doc object may also recieve a \"Start of sentence \" tag. while this doesent immediately build a list of senetenceds. these tagss enable genetration of semectences segements thourgh Doc.sents Later will write our own Segemnetation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3f313f40-1244-4fc3-9c81-bd0bb925e222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence\n"
     ]
    }
   ],
   "source": [
    "for sent in doc4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "472b3f7c-bae9-473e-912b-880f1171e80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We're moving to L.A.!\"\n"
     ]
    }
   ],
   "source": [
    "#Cretae a string which includes opening and closing quotatiton marks.\n",
    "mystring='\"We\\'re moving to L.A.!\"'\n",
    "print(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d984dc0b-75e9-4f4b-91f1-b8883c1b05e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" | We | 're | moving | to | L.A. | ! | \" | "
     ]
    }
   ],
   "source": [
    "#Create a Doc object and explore tokens\n",
    "doc=nlp(mystring)\n",
    "for token in doc:\n",
    "    print(token.text,end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fa8f6133-903c-498b-baae-0de5cdd04650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We |'re |here |to |help |! |Send |snail |- |mail |, |email |support@oursite.com |or |visit |us |at |http://www.oursite.com |! |"
     ]
    }
   ],
   "source": [
    "doc2=nlp(u\"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text,end=' |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0bc324-e16e-4d52-a15e-9d9bd476502f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f326fec1-58fe-4c4a-8c1b-aa644548d993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "NYC\n",
      "cab\n",
      "ride\n",
      "costs\n",
      "$\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "doc3=nlp(u'A 5km NYC cab ride costs $100')\n",
    "for t in doc3:\n",
    "    print(t)\n",
    "#here km and $  are preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf39c403-0706-495a-a4da-1b287c9a008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "visit\n",
      "St.\n",
      "Louis\n",
      "in\n",
      "the\n",
      "U.S.\n",
      "next\n",
      "year\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc4=nlp(u\"Let's visit St.Louis in the U.S. next year.\")\n",
    "for t in doc4:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdda466-e32d-46b4-aecb-54ac7ea9bc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple | to | build | a | Hong | Kng | factory | for | $ | 6 | million | \n",
      "------\n",
      "Apple-Companies, agencies, institutions, etc.\n",
      "Hong Kng-Countries, cities, states\n",
      "$6 million-Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "#29/08/24\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "doc8=nlp(u'Apple to build a Hong Kng factory for $6 million')\n",
    "for token in doc8:\n",
    "    print(token.text,end=' | ')\n",
    "\n",
    "print('\\n------')\n",
    "\n",
    "for ent in doc8.ents:\n",
    "    print(ent.text+'-'+str(spacy.explain(ent.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ec6fa2-163f-4f28-bdbc-060e408a5855",
   "metadata": {},
   "source": [
    "Noun chunks noun chunkls are similar to Doc.ents noun_chunks are another object pro[perty . noun chunks are 'Base noun phrases' first phrases that have a noun as their head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7f33af9-61be-4961-b9d0-be545e867989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liablity\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc9=nlp(u'Autonomous cars shift insurance liablity towards manufacturers')\n",
    "for chunk in doc9.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4b9a5c8-a09b-413b-b523-2e7cf6f60e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red cars\n",
      "higher insurance rates\n"
     ]
    }
   ],
   "source": [
    "doc10=nlp(u'Red cars do not carry higher insurance rates')\n",
    "for chunk in doc10.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeb8767-474f-4aef-8f6c-4308fb391090",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be7279-07eb-4bcb-9d9a-2d1de60b4bb3",
   "metadata": {},
   "source": [
    "Stemming is a somewhat crude method for cataloging related words. It essentially chops off letters from the end until the stem is reached. While it works fairly well in most cases, English has many exceptions where a more sophisticated process is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "360bf80f-1007-486f-ab78-a24e5320c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the toolkit and the full Porter Stemmer\n",
    "import nltk\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67f5fdfb-9f44-4a18-98a8-ed868067fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33999eef-a294-4a3b-a475-36c770983062",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['run','runner','running','ran','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0deb57f-d090-4298-bfed-bef00ae57fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run--->run\n",
      "runner--->runner\n",
      "running--->run\n",
      "ran--->ran\n",
      "easily--->easili\n",
      "fairly--->fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'--->'+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d91c003b-57b1-47d8-8c41-1e5b40e0e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball  import SnowballStemmer\n",
    "\n",
    "#The Snaowball stemmer requires that you pass a language parameter\n",
    "s_stemmer=SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a865afdc-275c-4583-a06b-8eb65b40dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['run','runner','running','ran','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10f4d156-932a-4ae5-8f66-a23159014be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run--->run\n",
      "runner--->runner\n",
      "running--->run\n",
      "ran--->ran\n",
      "easily--->easili\n",
      "fairly--->fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'--->'+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff8299-e630-4088-95a5-d1e9e0547364",
   "metadata": {},
   "source": [
    "Stemming has drawbacks.saw-->saw\n",
    "Lemmatization.saw-->saw or see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d21a992-7f8e-43e4-9b96-b3f7d9af6c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I--->i\n",
      "am--->am\n",
      "meeting--->meet\n",
      "him--->him\n",
      "tommorrow--->tommorrow\n",
      "at--->at\n",
      "the--->the\n",
      "meeting--->meet\n"
     ]
    }
   ],
   "source": [
    "phrase='I am meeting him tommorrow at the meeting'\n",
    "for word in phrase.split():\n",
    "    print(word+'--->'+p_stemmer.stem(word))\n",
    "\n",
    "#last meeting is also stemmed because it considers it has verb but it is nit verb it is noun...it is drawback\n",
    "#Therefore we use Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5696b-dfa5-4f77-89bd-bb9327e09ed5",
   "metadata": {},
   "source": [
    "In contrast to stemming, lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s full vocabulary to apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca49eed-e560-456e-9846-25f9a328169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "T \t PROPN \t 5582244037879929967 \t T\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "doc1=nlp(u'I am a runner running in a race because I love since T ran today')\n",
    "for token in doc1:\n",
    "    print(token.text,'\\t',token.pos_,'\\t',token.lemma,'\\t',token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca80302-c008-4440-a985-037f7d1754e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above code is staggred and hard to read.\n",
    "#Let's create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d310d989-0eb4-4e63-a69c-1a8250778427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_:{10}}')\n",
    "#Here we are using f string to format the printed text setting minimum fields widths and adding left asign to the lemma hash value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40feeaed-8640-4fe0-a3e3-386e7c0748cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I         \n",
      "saw          VERB   11925638236994514241   see       \n",
      "eighteen     NUM    9609336664675087640    eighteen  \n",
      "mice         NOUN   1384165645700560590    mouse     \n",
      "today        NOUN   11042482332948150395   today     \n"
     ]
    }
   ],
   "source": [
    "doc1=nlp(u'I saw eighteen mice today')\n",
    "show_lemma(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "977ac46a-5b68-4b8c-8cc5-01b48e56bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   4690420944186131903    I         \n",
      "am           AUX    10382539506755952630   be        \n",
      "meeting      VERB   6880656908171229526    meet      \n",
      "him          PRON   1655312771067108281    he        \n",
      "tommorrow    VERB   15839087253969881949   tommorrow \n",
      "at           ADP    11667289587015813222   at        \n",
      "the          DET    7425985699627899538    the       \n",
      "meeting      NOUN   14798207169164081740   meeting   \n"
     ]
    }
   ],
   "source": [
    "doc37=nlp(u'I am meeting him tommorrow at the meeting')\n",
    "show_lemmas(doc37)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9fa131-3556-47da-9672-488ef0bf25cf",
   "metadata": {},
   "source": [
    "Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee50c963-4474-4712-834a-9f95ee4ca839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform standard imports\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32a6ad-b312-49c0-9b1e-0cc66c983244",
   "metadata": {},
   "source": [
    "StopWords\n",
    "Stop words like ‘the’, ‘and’, and ‘I’, although common, don’t usually provide meaningful information about a document’s specific topic. By eliminating these words from a corpus, we can more easily identify unique and relevant terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "065c5baf-d3b6-4931-81d0-f546cc504117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'’ll', 'against', 'most', '‘re', \"'re\", 'nine', 'keep', 'none', 'above', 'him', 'they', 'beside', 'onto', 'ourselves', 'former', 'so', 'sometime', 'rather', 'then', 'may', 'until', 'perhaps', 'her', 'between', 'which', 'whereafter', 'side', '’d', 'various', 'whenever', 'own', 'through', 'whereupon', 'themselves', 'anyone', 'no', 'seems', 'latter', 'when', 'next', 'and', 'she', 'top', 'back', 'still', 'except', 'therein', 'move', 'it', 'do', 'not', '‘s', 'twelve', 'for', 'made', 'himself', 'before', 'nowhere', 'your', 'should', 'either', 'thus', 'seemed', 'everyone', 'of', 'one', 'sometimes', 'amongst', 'what', 'anyway', 'become', 'hundred', 'about', 'his', 'me', 'into', 'three', 'via', 'ours', 'indeed', 'nevertheless', 'yourself', 're', 'afterwards', 'call', 'else', 'see', 'nothing', 'could', 'upon', 'put', 'empty', 'even', 'now', 'had', 'were', '’ve', 'itself', 'always', 'almost', 'first', '’re', 'whereas', 'under', 'hereby', 'below', 'everywhere', 'say', 'each', 'few', 'down', 'out', 'herself', 'whither', 'take', 'get', 'alone', 'wherein', 'mine', 'has', 'thereupon', 'but', 'fifty', 'those', 'using', 'someone', \"'d\", 'if', 'done', 'will', 'sixty', 'therefore', 'a', 'beforehand', \"'ve\", 'anywhere', 'throughout', 'too', \"'s\", 'thereafter', 'just', 'across', 'third', 'very', 'wherever', 'eleven', 'several', 'i', 'them', 'us', 'really', 'name', 'unless', 'due', 'yourselves', 'latterly', 'besides', \"'ll\", 'whatever', 'thence', 'five', 'be', 'enough', 'behind', 'however', 'seeming', 'such', 'have', 'per', 'ever', 'along', 'please', 'nor', 'can', 'my', 'you', 'because', 'used', 'any', \"'m\", 'further', 'the', 'two', 'anyhow', 'other', 'whole', 'part', 'cannot', 'amount', '‘m', '’s', 'would', 'elsewhere', 'regarding', 'being', 'together', 'btw', 'its', 'something', 'myself', 'after', 'whom', 'more', 'well', 'both', 'does', 'an', 'thru', 'neither', 'on', 'towards', 'was', 'than', 'becomes', \"n't\", 'fifteen', 'ca', 'forty', 'as', 'some', 'these', 'again', 'bottom', 'show', '‘ll', 'much', 'namely', 'here', 'only', 'who', 'make', 'since', 'often', 'once', 'doing', 'ten', 'from', 'seem', 'where', 'to', 'hereupon', 'meanwhile', 'somehow', 'noone', 'mostly', 'least', 'in', 'go', 'beyond', 'although', 'am', 'off', '‘d', 'whether', 'up', 'give', 'within', 'with', 'everything', 'over', 'must', 'he', 'anything', 'hers', 'eight', 'yours', 'though', 'n‘t', 'many', 'might', 'this', 'moreover', '‘ve', '’m', 'yet', 'every', 'there', 'why', 'whereby', 'otherwise', 'at', 'already', 'full', 'became', 'did', 'by', 'whoever', 'also', 'we', 'whose', 'formerly', 'last', 'six', 'their', 'all', 'twenty', 'while', 'whence', 'how', 'hence', 'n’t', 'are', 'thereby', 'that', 'herein', 'been', 'around', 'our', 'serious', 'less', 'same', 'somewhere', 'never', 'quite', 'another', 'toward', 'among', 'without', 'or', 'hereafter', 'during', 'nobody', 'is', 'front', 'four', 'others', 'becoming'}\n"
     ]
    }
   ],
   "source": [
    "#print default stopwords\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd21b097-9e8f-4074-b084-e4d5ee5b0ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc833a45-0297-4de4-8e12-0a485e752740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To see if a word is a stopword or not\n",
    "nlp.vocab['myself'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "557d1dff-95e5-48e7-b7e6-8a6004ba1e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['magic'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42bb8c78-6c5d-441c-b9af-56abc3505541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To add a stopword\n",
    "#Add the word to the set of stop words Use lowercase\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "\n",
    "#Set the stop_words tag on the lexeme\n",
    "nlp.vocab['btw'].is_stop=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08c264bb-8001-4f02-bc33-55288c534ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09677a68-cd6f-484e-b186-9786579c2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To remove a stop words\n",
    "nlp.Defaults.stop_words.remove('beyond')\n",
    "\n",
    "#Remove thw stop_words tag from the lexeme\n",
    "nlp.vocab['beyond'].is_stop=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbaadbbc-e204-45e9-af80-299666fe3849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fb7a4a-6431-4ea2-95d9-74b6883113a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
